<!DOCTYPE html>
<html>
<head>
    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>The Markov Property, Chain, Reward Process and Decision Process</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <!-- highlight.js -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/vs2015.min.css">
    <style>.hljs { background: none; }</style>

    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="My thoughts, tutorials and learnings" />
    <link rel="shortcut icon" href="/assets/images/favicon.png" type="image/png" />
    <link rel="canonical" href="/markov-property-chain-reward-decision" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Xavier Geerinck - Blog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="The Markov Property, Chain, Reward Process and Decision Process" />
    <meta property="og:description" content="As seen in the previous article, we now know the general concept of Reinforcement Learning. But how do we actually get towards solving our third challenge: “Temporal Credit Assignment”? To solve this, we first need to introduce a generalization of our reinforcement models. When we look at these models, we" />
    <meta property="og:url" content="/markov-property-chain-reward-decision" />
    <meta property="og:image" content="/assets/images/covers/decision.png" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta property="article:author" content="https://www.facebook.com/" />
    <meta property="article:published_time" content="2018-05-20T09:00:00+00:00" />
    <meta property="article:modified_time" content="2018-05-20T09:00:00+00:00" />
    <meta property="article:tag" content="Ai" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="The Markov Property, Chain, Reward Process and Decision Process" />
    <meta name="twitter:description" content="As seen in the previous article, we now know the general concept of Reinforcement Learning. But how do we actually get towards solving our third challenge: “Temporal Credit Assignment”? To solve this, we first need to introduce a generalization of our reinforcement models. When we look at these models, we" />
    <meta name="twitter:url" content="/" />
    <meta name="twitter:image" content="/assets/images/covers/decision.png" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Xavier Geerinck - Blog" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="Ai" />
    <meta name="twitter:site" content="@XavierGeerinck" />
    <meta name="twitter:creator" content="@XavierGeerinck" />
    <meta property="og:image:width" content="1400" />
    <meta property="og:image:height" content="933" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Xavier Geerinck - Blog",
        "logo": "/false"
    },
    "url": "/markov-property-chain-reward-decision",
    "image": {
        "@type": "ImageObject",
        "url": "/assets/images/covers/decision.png",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "/markov-property-chain-reward-decision"
    },
    "description": "As seen in the previous article, we now know the general concept of Reinforcement Learning. But how do we actually get towards solving our third challenge: “Temporal Credit Assignment”? To solve this, we first need to introduce a generalization of our reinforcement models. When we look at these models, we"
}
    </script>

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="The Markov Property, Chain, Reward Process and Decision Process" href="/feed.xml" />



    <!-- Enable MathJAX for LaTeX equations -->
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
        });
        </script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-about" role="menuitem"><a href="/projects">Projects</a></li>
    <li class="nav-about" role="menuitem"><a href="/demos">Demos</a></li>
    <li class="nav-about" role="menuitem"><a href="/categories">Categories</a></li>
</ul>

    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
                <a class="social-link social-link-tw" href="https://twitter.com/XavierGeerinck" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>
</a>
            
            
                <a class="social-link social-link-tw" href="https://linkedin.com/in/xaviergeerinck" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/></svg></a>
            
        </div>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full post ">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime="20 May 2018">20 May 2018</time>
                    
                        <span class="date-divider">/</span>
                        
                            
                               <a href='/tag/ai/'>AI</a>,
                            
                        
                            
                               <a href='/tag/ai-ml/'>AI-ML</a>,
                            
                        
                            
                               <a href='/tag/ai-rl/'>AI-RL</a>
                            
                        
                    
                </section>
            </header>

            
            <figure class="post-full-image" style="background-image: url(/assets/images/covers/decision.png)">
            </figure>
            

            <section class="post-full-content">
                <div class="kg-card-markdown">
                        <h1 class="post-full-title">The Markov Property, Chain, Reward Process and Decision Process</h1>
                    <p>As seen in the previous article, we now know the general concept of Reinforcement Learning. But how do we actually get towards <strong>solving our third challenge: “Temporal Credit Assignment”</strong>?</p>

<p>To solve this, we first need to introduce a generalization of our reinforcement models. When we look at these models, we can see that we are modeling decision-making situations where the outcomes of these situations are partly random and partly under the control of the decision maker. This is what we call the <strong>Markov Decision Process</strong> or <strong>MDP</strong> - we say that it satisfies the <strong>Markov Property</strong>. But let’s go a bit deeper in this.</p>

<h2 id="markov-decision-processes">Markov Decision Processes</h2>

<h3 id="markov-property">Markov Property</h3>

<p>If our state representation is as effective as having a full history, then we say that our model fulfills the requirements of the Markov Property.</p>

<p>To illustrate this with an example, think of playing Tic-Tac-Toe. When we are able to take a decision based on the current state, rather than needing to know the whole history, then we say that we satisfy the conditions of the Markov Property.</p>

<p>Or in more general terms:</p>

<p><em><center><span style="font-size: 24px;">"The future is independent of the past given the present"</span></center></em></p>

<p>We say that we can go from one Markov State $s$ to the successor state $s’$ by defining the state transition probability, which is defined by $P_{ss’} = P[S_{t+1} = s’ \mid S_t = s]$.</p>

<h3 id="markov-process-or-markov-chain">Markov Process or Markov Chain</h3>

<p>A Markov Process is a memoryless random process where we take a sequence of random states that fulfill the Markov Property requirements. Or in a <em>definition</em>:</p>

<p>A Markov Process is a tuple &lt;$S$, $P$&gt; where:</p>

<ul>
  <li>$S$ is a (finite) set of states</li>
  <li>$P$ is a state transition probability matrix, $P_{ss’} = P[S_{t+1} = s’ \mid S_t = s]$.</li>
</ul>

<p>With our $P$ matrix being written as:</p>

<script type="math/tex; mode=display">% <![CDATA[
P = \begin{bmatrix}P_{11} & ... & P_{1n} \\ \vdots & ... & \vdots \\ P_{n1} & ... & P_{nn} \\ \end{bmatrix} %]]></script>

<p>where each row of the matrix sums to 1.</p>

<p>Let’s illustrate this with an example. Let’s say that we want to represent weather conditions. How can we predict the weather on the following days?</p>

<p>When we have this transition matrix:</p>

<script type="math/tex; mode=display">% <![CDATA[
P = \begin{bmatrix}0.9 & 0.1 \\ 0.5 & 0.5\end{bmatrix} %]]></script>

<p>Then we can see that we will have a 90% chance of a sunny day following on a current sunny day and a 50% chance of a rainy day when we currently have a rainy day.</p>

<p>Representing this as a graph results in:</p>

<p><img src="assets/images/posts/markov-chain.png" alt="assets/images/posts/markov-chain.png" /></p>

<h3 id="markov-reward-process-mrp">Markov Reward Process (MRP)</h3>

<p>To come to the fact of taking decisions, as we do in Reinforcement Learning. We introduce something called “reward”. This will help us choose an action, based on the current environment and the reward we will get for it.</p>

<p>The Markov Reward Process is an extension on the original Markov Process, but with adding rewards to it. Written in a <strong>definition</strong>:</p>

<p>A Markov Reward Process is a tuple &lt;$S$, $P$, $R$, $γ$&gt; where:</p>

<ul>
  <li>$S$ is a (finite) set of states</li>
  <li>$P$ is a state transition probability matrix, $P_{ss’} = P[S_{t+1} = s’ \mid S_t = s]$.</li>
  <li><strong>$R$ is a reward function, $R_s = E[r_{t+1} \mid S_t = s]$</strong></li>
  <li><strong>$γ$ is a discount factor, $γ \in [0,1]$</strong></li>
</ul>

<p>Which means that we will add a reward of going to certain states. When we map this on our earlier example:</p>

<p><img src="assets/images/posts/markov-reward-process.png" alt="assets/images/posts/markov-reward-process.png" /></p>

<p>By adding this reward, we can find an optimal path for a couple of days when we are in the lead of deciding. Let’s imagine that we can play god here, what path would you take? Well we would like to try and take the path that stays “sunny” the whole time, but why? Well because that means that we would end up with the highest reward possible.</p>

<h4 id="return">Return</h4>

<p>But how do we calculate the complete return that we will get? Well this is represented by the following formula:</p>

<script type="math/tex; mode=display">G_t = R_{t+1} + R_{t+2} + ... + R_n</script>

<p>This however results in a couple of problems:</p>

<ul>
  <li>We tend to stop exploring (we choose the option with the highest reward every time)</li>
  <li>Possibility of infinite returns in a cyclic Markov Process</li>
</ul>

<p>Which is why we added a new factor called the <em>discount factor</em>. This factor will decrease the reward we get of taking the same action over time. Adding this to our original formula results in:</p>

<script type="math/tex; mode=display">G_t = R_{t+1} + γR_{t+2} + ... + γ^nR_n = \sum^{\infty}_{k=0}γ^kR_{t + k + 1}</script>

<h3 id="markov-decision-process-mdp">Markov Decision Process (MDP)</h3>

<p>A Markov Decision Process is a Markov reward process with decisions. It is an environment in which all states are Markov.</p>

<p>We can now finalize our definition towards:</p>

<p>A Markov Decision Process is a tuple &lt;$S$, $A$, $P$, $R$, $γ$&gt; where:</p>

<ul>
  <li>$S$ is a (finite) set of states</li>
  <li><strong>$A$ is a finite set of actions</strong></li>
  <li>$P$ is a state transition probability matrix, $P_{ss’}^a = P[S_{t+1} = s’ \mid S_t = s, A_t = a]$.</li>
  <li>$R$ is a reward function, $R_s^a = E[r_{t+1} \mid S_t = s, A_t = a]$</li>
  <li>$γ$ is a discount factor, $γ \in [0,1]$</li>
</ul>

<p><img src="assets/images/posts/markov-decision-process.png" alt="assets/images/posts/markov-decision-process.png" /></p>

<h2 id="references">References</h2>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Markov_property">https://en.wikipedia.org/wiki/Markov_property</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Markov_decision_process">https://en.wikipedia.org/wiki/Markov_decision_process</a></li>
  <li><a href="https://stats.stackexchange.com/questions/221402/understanding-the-role-of-the-discount-factor-in-reinforcement-learning">https://stats.stackexchange.com/questions/221402/understanding-the-role-of-the-discount-factor-in-reinforcement-learning</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Bellman_equation">https://en.wikipedia.org/wiki/Bellman_equation</a></li>
  <li><a href="https://homes.cs.washington.edu/~todorov/courses/amath579/MDP.pdf">https://homes.cs.washington.edu/~todorov/courses/amath579/MDP.pdf</a></li>
  <li><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Examples_of_Markov_chains">https://en.wikipedia.org/wiki/Examples_of_Markov_chains</a></li>
</ul>

                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
            

            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                    
                        <section class="author-card">
                            
                                <img class="author-profile-image" src="/assets/images/avatar.jpg" alt="xavier" />
                            
                            <section class="author-card-content">
                                <h4 class="author-card-name"><a href="/author/xavier">Xavier Geerinck</a></h4>
                                
                                    <p>Xavier works as a Cloud Solution Architect at Microsoft, helping its customer unlock the full potential of the cloud. Even though he is still considered a young graduate, he achieved his first success at the age 16, by creating and selling his first startup. He then took this knowledge to create and help more startups in different markets such as technology, social media, philanthropy and home care. While in the meantime gaining more enterprise insights at renowned enterprises such as Nokia, Cisco and now Microsoft.</p>
                                
                            </section>
                        </section>
                        <div class="post-full-footer-right">
                            <a class="author-card-button" href="/author/xavier">Read More</a>
                        </div>
                    
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            
                <section class="post-full-comments">
                    <div id="disqus_thread"></div>
                    <script>
                        var disqus_config = function () {
                            this.page.url = window.location.href;
                            this.page.identifier = '/markov-property-chain-reward-decision';
                        };
                        (function() {
                            var d = document, s = d.createElement('script');
                            s.src = 'https://xaviergeerinck.disqus.com/embed.js';
                            s.setAttribute('data-timestamp', +new Date());
                            (d.head || d.body).appendChild(s);
                        })();
                    </script>
                </section>
            

        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            
                
                
                
                
                    <article class="read-next-card"
                        
                            style="background-image: url(/assets/images/covers/blog-cover.png)"
                        
                    >
                        <header class="read-next-card-header">
                            <small class="read-next-card-header-sitetitle">&mdash; Xavier Geerinck - Blog &mdash;</small>
                            
                                <h3 class="read-next-card-header-title"><a href="/tag/ai/">Ai</a></h3>
                            
                        </header>
                        <div class="read-next-divider"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 14.5s2 3 5 3 5.5-2.463 5.5-5.5S21 6.5 18 6.5c-5 0-7 11-12 11C2.962 17.5.5 15.037.5 12S3 6.5 6 6.5s4.5 3.5 4.5 3.5"/></svg>
</div>
                        <div class="read-next-card-content">
                            <ul>
                                
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/facebook-reagent">Facebook ReAgent - An End-to-End Use Case</a></li>
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/facebook-horizon">Facebook's Open-Source Reinforcement Learning Platform - A Deep Dive</a></li>
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                            <li><a href="/rl-overview-terminology">Reinforcement Learning - Terminology</a></li>
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                    
                                        
                                        
                                    
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                                  
                                
                            </ul>
                        </div>
                        <footer class="read-next-card-footer">
                            <a href="/tag/ai/">
                                
                                    See all 10 posts  →
                                
                            </a>
                        </footer>
                    </article>
                
            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            
                
    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/ordinary-least-squares">
                <div class="post-card-image" style="background-image: url(/assets/images/covers/algorithms3.jpg)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/ordinary-least-squares">
                <header class="post-card-header">
                    
                        
                            
                                <span class="post-card-tags">Algorithms</span>
                            
                        
                    

                    <h2 class="post-card-title">Ordinary Least Squares (OLS)</h2>
                </header>
                <section class="post-card-excerpt">
                    <p>Let’s start by defining the goal of our algorithm, what do we want to achieve with our OLS algorithm? Well if we have data points in a region (or XY-axis), then we want</p>
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                        
                        <img class="author-profile-image" src="/assets/images/avatar.jpg" alt="Xavier Geerinck" />
                        
                        <span class="post-card-author">
                            <a href="/author/xavier/">Xavier Geerinck</a>
                        </span>
                    
                
            </footer>
        </div>
    </article>

            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            
                
    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/bellman-equations">
                <div class="post-card-image" style="background-image: url(/assets/images/covers/rl3.png)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/bellman-equations">
                <header class="post-card-header">
                    
                        
                            
                               <span class="post-card-tags">Ai</span>
                            
                        
                            
                               <span class="post-card-tags">Ai-ml</span>
                            
                        
                            
                                <span class="post-card-tags">Ai-rl</span>
                            
                        
                    

                    <h2 class="post-card-title">Bellman Equations</h2>
                </header>
                <section class="post-card-excerpt">
                    <p>## Summary So what did we learn up until now in our [Introduction](/rl-intro) and [Markov Property, Chain, Reward Process and Decision Process](/markov-property-chain-reward-decision) posts? Initially we defined our basic concepts: * **State:** What does</p>
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                        
                        <img class="author-profile-image" src="/assets/images/avatar.jpg" alt="Xavier Geerinck" />
                        
                        <span class="post-card-author">
                            <a href="/author/xavier/">Xavier Geerinck</a>
                        </span>
                    
                
            </footer>
        </div>
    </article>

            
        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="/">
            
                <img src="/assets/images/favicon.png" alt="Xavier Geerinck - Blog icon" />
            
            <span>Xavier Geerinck - Blog</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">The Markov Property, Chain, Reward Process and Decision Process</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=The+Markov+Property%2C+Chain%2C+Reward+Process+and+Decision+Process&amp;url=https://thebillkidy.github.io/markov-property-chain-reward-decision"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>

        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=https://thebillkidy.github.io/markov-property-chain-reward-decision"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>

        </a>
        <a class="floating-header-share-li" href="https://www.linkedin.com/shareArticle?mini=true&url=https://thebillkidy.github.io/markov-property-chain-reward-decision&title=The+Markov+Property%2C+Chain%2C+Reward+Process+and+Decision+Process&source=https://thebillkidy.github.io/"
            onclick="window.open(this.href, 'share-linkedin','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/></svg>
        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>


<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->


        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="/">Xavier Geerinck - Blog</a> &copy; 2019</section>
                <section class="poweredby">Made with ❤ in Belgium</section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    <a href="https://twitter.com/XavierGeerinck" target="_blank" rel="noopener">Twitter</a>
                    <a href="https://linkedin.com/in/xaviergeerinck" target="_blank" rel="noopener">LinkedIn</a>
                </nav>
            </div>
        </footer>
    </div>

    <!-- The big email subscribe modal content -->
     

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/scala.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
    <!-- <script>hljs.initHighlightingOnLoad();</script> -->

    <script>
        var blocks = document.querySelectorAll('div[class^=language-]');
        blocks.forEach((b) => {
            var language = b.className.split(" ")[0].match(/language\-(.*)/)[1];

            var codeBlocks = b.querySelectorAll('pre code');

            // Highlight the code
            codeBlocks.forEach((i) => {
                var text = i.innerText;
                var highlightedText = hljs.highlight(language, text);
                i.innerHTML = highlightedText.value;
            });

            // Add line numbers

            codeBlocks.forEach((i) => {            
                i.innerHTML = i.innerHTML.split("\n").map(function (i, idx, arr) {
                    // The last line seems to be empty, so don't work with it
                    if (idx == arr.length - 1) {
                        return '';
                    }

                    return '<span class="line-numbers"">' + i + '</span>' + "\n";
                }).join("");
            });
        });
    </script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.3.1.min.js"
        integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
        crossorigin="anonymous"></script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/fitvids/1.2.0/jquery.fitvids.min.js"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-27398198-1', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

</body>
</html>
